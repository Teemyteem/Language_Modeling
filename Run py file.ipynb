{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load C:\\\\Users\\\\TEEM\\\\Desktop\\\\shakespears\\\\dataset.py\n",
    "# import some packages you need here\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "class Shakespeare(Dataset):\n",
    "    \"\"\" Shakespeare dataset\n",
    "\n",
    "        To write custom datasets, refer to\n",
    "        https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "    Args:\n",
    "        input_file: txt file\n",
    "\n",
    "    Note:\n",
    "        1) Load input file and construct character dictionary {index:character}.\n",
    "           You need this dictionary to generate characters.\n",
    "        2) Make list of character indices using the dictionary\n",
    "        3) Split the data into chunks of sequence length 30. \n",
    "           You should create targets appropriately.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_file):\n",
    "\n",
    "        # write your codes here\n",
    "        with open(input_file, 'r') as file:\n",
    "            self.text = file.read()\n",
    "            \n",
    "        # construct character dictionary {index:character}\n",
    "        self.chars = sorted(set(self.text))\n",
    "        self.char2idx = {char: idx for idx, char in enumerate(self.chars)}\n",
    "        self.idx2char = {idx: char for idx, char in enumerate(self.chars)}\n",
    "        \n",
    "        # character indices\n",
    "        self.text_indices = [self.char2idx[char] for char in self.text]\n",
    "        \n",
    "        # sequence length\n",
    "        self.seq_length = 30\n",
    "        \n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        for i in range(0, len(self.text_indices) - self.seq_length):\n",
    "            self.data.append(self.text_indices[i:i + self.seq_length])\n",
    "            self.targets.append(self.text_indices[i + 1:i + self.seq_length + 1])    \n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "\n",
    "        # write your codes here\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # write your codes here\n",
    "        input_sq = torch.tensor(self.data[idx], dtype=torch.long)\n",
    "        target_sq = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "        return input_sq, target_sq\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # write test codes to verify your implementations\n",
    "    dataset = Shakespeare('C:\\\\Users\\\\TEEM\\\\Desktop\\\\shakespears\\\\shakespeare_train.txt') \n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load C:\\\\Users\\\\TEEM\\\\Desktop\\\\shakespears\\\\model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, dropout=0.5):\n",
    "\n",
    "        # write your codes here\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        # write your codes here\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output.reshape(output.size(0) * output.size(1), output.size(2)))\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        # write your codes here\n",
    "        initial_hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        \n",
    "        return initial_hidden\n",
    "\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, dropout=0.5):\n",
    "\n",
    "        # write your codes here\n",
    "        super(CharLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        # write your codes here\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output.reshape(output.size(0) * output.size(1), output.size(2)))\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        # write your codes here\n",
    "        hidden_state = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        cell_state = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        \n",
    "        return (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76939c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load C:\\\\Users\\\\TEEM\\\\Desktop\\\\shakespears\\\\generate.py\n",
    "# import some packages you need here\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(model, seed_characters, temperature, device, char_to_idx, idx_to_char, length=100):\n",
    "    \"\"\" Generate characters\n",
    "\n",
    "    Args:\n",
    "        model: trained model\n",
    "        seed_characters: seed characters\n",
    "        temperature: T\n",
    "        args: other arguments if needed\n",
    "\n",
    "    Returns:\n",
    "        samples: generated characters\n",
    "    \"\"\"\n",
    "\n",
    "    # write your codes here\n",
    "    model.eval()\n",
    "    samples = seed_characters\n",
    "    input_seq = torch.tensor([char_to_idx[char] for char in seed_characters], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            output, hidden = model(input_seq[:, -1:], hidden)\n",
    "            output = output.squeeze().div(temperature).exp()\n",
    "            probs = F.softmax(output, dim=-1).cpu().numpy()\n",
    "            next_index = np.random.choice(probs.size, p=probs)\n",
    "            next_char = idx_to_char[next_index]\n",
    "            samples += next_char\n",
    "            input_seq = torch.cat([input_seq, torch.tensor([[next_index]], dtype=torch.long).to(device)], dim=1)\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2669b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load C:\\\\Users\\\\TEEM\\\\Desktop\\\\shakespears\\\\main.py\n",
    "import dataset\n",
    "from model import CharRNN, CharLSTM\n",
    "\n",
    "# import some packages you need here\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import Shakespeare\n",
    "from model import CharRNN, CharLSTM\n",
    "from generate import generate\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "def train(model, trn_loader, device, criterion, optimizer):\n",
    "    \"\"\" Train function\n",
    "\n",
    "    Args:\n",
    "        model: network\n",
    "        trn_loader: torch.utils.data.DataLoader instance for training\n",
    "        device: device for computing, cpu or gpu\n",
    "        criterion: cost function\n",
    "        optimizer: optimization method, refer to torch.optim\n",
    "\n",
    "    Returns:\n",
    "        trn_loss: average loss value\n",
    "    \"\"\"\n",
    "\n",
    "    # write your codes here\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in trn_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(inputs, model.init_hidden(inputs.size(0)))\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    trn_loss = total_loss / len(trn_loader)\n",
    "\n",
    "    return trn_loss\n",
    "\n",
    "def validate(model, val_loader, device, criterion):\n",
    "    \"\"\" Validate function\n",
    "\n",
    "    Args:\n",
    "        model: network\n",
    "        val_loader: torch.utils.data.DataLoader instance for testing\n",
    "        device: device for computing, cpu or gpu\n",
    "        criterion: cost function\n",
    "\n",
    "    Returns:\n",
    "        val_loss: average loss value\n",
    "    \"\"\"\n",
    "\n",
    "    # write your codes here\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs, _ = model(inputs, model.init_hidden(inputs.size(0)))\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    val_loss = total_loss / len(val_loader)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" Main function\n",
    "\n",
    "        Here, you should instantiate\n",
    "        1) DataLoaders for training and validation. \n",
    "           Try SubsetRandomSampler to create these DataLoaders.\n",
    "        3) model\n",
    "        4) optimizer\n",
    "        5) cost function: use torch.nn.CrossEntropyLoss\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # write your codes here\n",
    "    input_file = 'C:\\\\Users\\\\TEEM\\\\Desktop\\\\shakespears\\\\shakespeare_train.txt'\n",
    "    batch_size = 128\n",
    "    sequence_length = 30\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.0001\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Dataset\n",
    "    dataset = Shakespeare(input_file)\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(0.2 * dataset_size))\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    trn_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # RNN\n",
    "    rnn_model = CharRNN(input_size=len(dataset.char2idx), hidden_size=128, output_size=len(dataset.char2idx), n_layers=2).to(device)\n",
    "    rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=learning_rate)\n",
    "    rnn_train_losses, rnn_val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        rnn_train_loss = train(rnn_model, trn_loader, device, criterion, rnn_optimizer)\n",
    "        rnn_val_loss = validate(rnn_model, val_loader, device, criterion)\n",
    "        rnn_train_losses.append(rnn_train_loss)\n",
    "        rnn_val_losses.append(rnn_val_loss)\n",
    " \n",
    "    \n",
    "    # LSTM\n",
    "    lstm_model = CharLSTM(input_size=len(dataset.char2idx), hidden_size=128, output_size=len(dataset.char2idx), n_layers=2).to(device)\n",
    "    lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "    lstm_train_losses, lstm_val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        lstm_train_loss = train(lstm_model, trn_loader, device, criterion, lstm_optimizer)\n",
    "        lstm_val_loss = validate(lstm_model, val_loader, device, criterion)\n",
    "        lstm_train_losses.append(lstm_train_loss)\n",
    "        lstm_val_losses.append(lstm_val_loss)\n",
    "        \n",
    "        \n",
    "    # Plotting\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    plt.plot(epochs, rnn_train_losses, label='RNN Train Loss')\n",
    "    plt.plot(epochs, rnn_val_losses, label='RNN Val Loss')\n",
    "    plt.plot(epochs, lstm_train_losses, label='LSTM Train Loss')\n",
    "    plt.plot(epochs, lstm_val_losses, label='LSTM Val Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Generate samples\n",
    "    seed_characters = \"The characters\"\n",
    "    temperature = 0.7\n",
    "    rnn_samples = generate(rnn_model, seed_characters, temperature, device, dataset.char2idx, dataset.idx2char)\n",
    "    lstm_samples = generate(lstm_model, seed_characters, temperature, device, dataset.char2idx, dataset.idx2char)\n",
    "    \n",
    "    print(\"\\nGenerated Samples with RNN:\\n\")\n",
    "    for sample in rnn_samples:\n",
    "        print(sample)\n",
    "    \n",
    "    print(\"\\nGenerated Samples with LSTM:\\n\")\n",
    "    for sample in lstm_samples:\n",
    "        print(sample)    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d592d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a6b6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b0f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
